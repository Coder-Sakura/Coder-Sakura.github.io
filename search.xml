<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python爬虫-pixiv关注画师作品[1]</title>
      <link href="/blog/2019/03/27/pixiv-one/"/>
      <url>/blog/2019/03/27/pixiv-one/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/03/27/pixiv-one/HeadPicture.jpg"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：大概有几篇是关于pixiv关注画师的作品抓取的思路和代码，后面大概还会有个个人收藏的抓取（这个比较简单）</p></blockquote><a id="more"></a><h2 id="1、pixiv介绍"><a href="#1、pixiv介绍" class="headerlink" title="1、pixiv介绍"></a>1、pixiv介绍</h2><blockquote><ul><li><a href="www.pixiv.net">Pixiv</a></li><li>这是一个墙外的网站，需要正确的扶梯姿势和科学上网</li><li>当然也可以参考我的方法（nginx + 修改 hosts 文件）</li><li>地址：<a href="https://www.lanzous.com/b649306/" target="_blank" rel="noopener">点击前往</a>  密码:3235 (建议 hosts 文件取自己需要的那部分)</li><li>为什么选 pixiv 呢？其实在12月份的时候我刚开始学 python，这个 pixiv 的小项目是我自己突发奇想要做的，那时候是新年前一周左右，对于那时候的我来说，pixiv 反爬难度一般般，但比较难的数据接口分析、构造url、文件操作、代理、图片合成等（自己一个人盯了这个网站几天，最后完成超级兴奋！）</li></ul></blockquote><h2 id="2、Target"><a href="#2、Target" class="headerlink" title="2、Target"></a>2、Target</h2><blockquote><ul><li>登录账号关注的画师的作品</li><li>思路：<ul><li>首先是模拟登录（PC 用过 pixiv 的同学都知道在未登录的时候 pixiv 会对用户做一些限制，所以我们要先模拟登录）</li><li>其次保持会话连接（可以考虑 cookie 保存，这里采用的是 requests 的 session 会话连接）</li><li>（基于图片网站，可能是动态加载，那么需要分析接口或者是 selenium 模拟）</li><li>最后才进行网页内容分析，然后抓取保存下来</li></ul></li></ul></blockquote><hr><h3 id="3、实现流程"><a href="#3、实现流程" class="headerlink" title="3、实现流程"></a>3、实现流程</h3><blockquote><h4 id="一、查找登录接口"><a href="#一、查找登录接口" class="headerlink" title="一、查找登录接口"></a>一、查找登录接口</h4></blockquote><ul><li>第一次找关于登录接口的时候，一个login都没看到，只看到一个 <a href="http://www.pixiv.net" target="_blank" rel="noopener">www.pixiv.net</a>     ,可惜是get请求的页面。</li><li>在拜读了 <a href="https://blog.csdn.net/df0128/article/details/80953212" target="_blank" rel="noopener">Chrome使用技巧</a> 、<a href="https://blog.csdn.net/Letasian/article/details/78461438" target="_blank" rel="noopener">Chrome开发者工具使用小技巧</a> 后，算是对 chrome 的调试工具有个大概了解的印象，知道了 preserve log 勾选后，可以保留网络日志，于是发现了真正的登录请求</li></ul><img src="/blog/2019/03/27/pixiv-one/1.png"><blockquote><p>分析参数</p></blockquote><p>password：个人密码</p><p>pixiv_id：个人id</p><p>post_key：不明字符串</p><p>source：pc即电脑端（截图没截全，把return_to漏掉了。。。）</p><p>return_to：是登录成功后跳转的页面，这个可以自己填，貌似默认是 <a href="https://www.pixiv.net/" target="_blank" rel="noopener">https://www.pixiv.net/</a></p><blockquote><p>那么接下来就是找post_key了</p></blockquote><ul><li>首先pixiv非常友好，所以应该不是js加密，而是在页面中随机生成的。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其次在点击登录的时候就跳转 url1 ↓</span></span><br><span class="line"><span class="comment"># url1 = https://accounts.pixiv.net/login?lang=zh&amp;source=pc&amp;view_type=page&amp;ref=wwwtop_accounts_index</span></span><br><span class="line"><span class="comment"># 但是登录请求的 url 是 url2 ↓</span></span><br><span class="line"><span class="comment"># url2 = https://accounts.pixiv.net/api/login?lang=zh</span></span><br><span class="line"><span class="comment"># 所以猜想 post_key 应该是在前者中生成的。</span></span><br></pre></td></tr></table></figure><blockquote><p>F12 打开，在 Elements 中 Ctrl + F 查看 post_key</p></blockquote><img src="/blog/2019/03/27/pixiv-one/2.png"><blockquote><p> 接下来用 BeautifulSoup 匹配</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.post_key = post_key_soup.find(<span class="string">'input'</span>) [<span class="string">'value'</span>]</span><br><span class="line"><span class="comment"># 因为是第一个input标签，而find返回的是第一个符合要求的结果</span></span><br></pre></td></tr></table></figure><blockquote><p>接着向 url2 发去 post 请求</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'pixiv_id'</span>: self.pixiv_id,</span><br><span class="line">    <span class="string">'password'</span>: self.password,</span><br><span class="line">    <span class="string">'return_to'</span>: self.return_to,</span><br><span class="line">    <span class="string">'post_key'</span>: self.post_key&#125;</span><br><span class="line">rep = se.post(self.login_url, data=data, headers=self.headers,verify=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># login_url是上面的 url2</span></span><br><span class="line"><span class="comment"># 我这里 return_to 写的是个人关注画师的那个页面的 url</span></span><br></pre></td></tr></table></figure><blockquote><p>登录代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">se = requests.session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">    post_key_html = self.request(self.base_url)<span class="comment"># base_url 是上面的 url1</span></span><br><span class="line">    post_key_soup = BeautifulSoup(post_key_html.text, <span class="string">'lxml'</span>)</span><br><span class="line">    self.post_key = post_key_soup.find(<span class="string">'input'</span>)[<span class="string">'value'</span>]</span><br><span class="line">    print(self.post_key)    <span class="comment">#捕获postkey</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">'pixiv_id'</span>: self.pixiv_id,</span><br><span class="line">        <span class="string">'password'</span>: self.password,</span><br><span class="line">        <span class="string">'return_to'</span>: self.return_to,</span><br><span class="line">        <span class="string">'post_key'</span>: self.post_key&#125;</span><br><span class="line">    rep = se.post(self.login_url, data=data, headers=self.headers,verify=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># login_url 是上面的 url2</span></span><br><span class="line">    print(<span class="string">'登陆成功'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>顺便吐槽下 HTTPS 的证书报警问题</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests.packages.urllib3.exceptions <span class="keyword">import</span> InsecureRequestWarning     <span class="comment">#强制取消警告</span></span><br><span class="line">requests.packages.urllib3.disable_warnings(InsecureRequestWarning)</span><br></pre></td></tr></table></figure><blockquote><p>先到这吧，明天继续写解析关注画师页面（页数），寻找数据接口，单图动图多图下载估计写不到了</p><p>へ(￣ ￣;へ) </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> lxml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Next主题集成 algolia 站内搜索插件</title>
      <link href="/blog/2019/03/26/algolia/"/>
      <url>/blog/2019/03/26/algolia/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/03/26/algolia/HeadPicture.jpg"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：Next主题集成algolia </p></blockquote><a id="more"></a><h3 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h3><ul><li>今天添加并完善了下blog的站内搜索（既然next有集成，为何不用呢？）</li><li>主要是参考了几篇超级详细的文章</li><li>大赞：<a href="http://www.qingpingshan.com/m/view.php?aid=386198" target="_blank" rel="noopener">Hexo+Next集成Algolia搜索</a>、<a href="https://www.zhihu.com/question/46822587" target="_blank" rel="noopener">知乎</a></li></ul><h3 id="二、问题枪毙名单"><a href="#二、问题枪毙名单" class="headerlink" title="二、问题枪毙名单"></a>二、问题枪毙名单</h3><h4 id="1-Not-enough-rights-to-update-an-object-near"><a href="#1-Not-enough-rights-to-update-an-object-near" class="headerlink" title="1. Not enough rights to update an object near"></a>1. Not enough rights to update an object near</h4><ul><li>解决方法：修改Algolia的ACL访问控制列表</li><li><img src="/blog/2019/03/26/algolia/1.png"></li><li><img src="/blog/2019/03/26/algolia/2.png"></li><li>将ACL修改为以上所示，文章里的ACL和现在的界面不一样，不知道是我用得少的原因(雾)，找了几分钟左右。</li></ul><h4 id="2-Please-provide-an-Algolia-index-name-in-your-hexo-config-yml-flle"><a href="#2-Please-provide-an-Algolia-index-name-in-your-hexo-config-yml-flle" class="headerlink" title="2. Please provide an Algolia index name in your hexo _config.yml flle"></a>2. Please provide an Algolia index name in your hexo _config.yml flle</h4><ul><li>解决方法：修改index名称</li><li>index名称就是在以下这个界面输入的那个index name</li><li><img src="/blog/2019/03/26/algolia/3.png"></li></ul><h4 id="3-ERROR-Algolia-Please-set-an-HEXO-ALGOLIA-INDEXING-KEY-environment-variable-to-enable-content-indexing"><a href="#3-ERROR-Algolia-Please-set-an-HEXO-ALGOLIA-INDEXING-KEY-environment-variable-to-enable-content-indexing" class="headerlink" title="3. ERROR [Algolia] Please set an HEXO_ALGOLIA_INDEXING_KEY environment variable to enable content indexing."></a>3. ERROR [Algolia] Please set an <code>HEXO_ALGOLIA_INDEXING_KEY</code> environment variable to enable content indexing.</h4><ul><li><p>这个通常是在hexo algolia的时候出现的问题</p></li><li><p>其实在上面的文章也有说到，这里简单说一下</p></li><li><p>解决方法：</p></li><li><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> HEXO_ALGOLIA_INDEXING_KEY=[你的API Key]</span><br></pre></td></tr></table></figure></li><li><p>API Key 是 Search-Only API key</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lxml&amp;Xpath</title>
      <link href="/blog/2019/03/13/xpath/"/>
      <url>/blog/2019/03/13/xpath/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/03/13/xpath/HeadPicture.png"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：Xpath语法，lxml解析html</p></blockquote><a id="more"></a><h3 id="一、什么是Xpath"><a href="#一、什么是Xpath" class="headerlink" title="一、什么是Xpath?"></a>一、什么是Xpath?</h3><ol><li>XPath 是 XML 路径语言，主要是在 XML 和 HTML 文档中查找我们想要的信息的语言。</li><li>XML 和 HTML 一样也是标记语言，但是 XML 用来传输和存储数据，而 HTML 用来显示数据</li></ol><h3 id="二、Xpath工具"><a href="#二、Xpath工具" class="headerlink" title="二、Xpath工具"></a>二、Xpath工具</h3><ol><li>Google：Xpath Helper</li></ol><p>（Google 插件可到下载<a href="https://www.crx4chrome.com/" target="_blank" rel="noopener">Crx4Chrom</a>(英文)、<a href="http://www.cnplugins.com/" target="_blank" rel="noopener">插件网</a>、<a href="http://chromecj.com/" target="_blank" rel="noopener">Chrome插件网 </a>下载）</p><ol><li>Firefox：Try Xpath</li><li>每个浏览器一般在应用中心或拓展里都可以下载</li></ol><blockquote><p>Xpath Helper 界面</p></blockquote><img src="/blog/2019/03/13/xpath/1.png"><h3 id="三、Xpath语法"><a href="#三、Xpath语法" class="headerlink" title="三、Xpath语法"></a>三、Xpath语法</h3><h4 id="1、路径表达式语法、相对-绝对路径"><a href="#1、路径表达式语法、相对-绝对路径" class="headerlink" title="1、路径表达式语法、相对/绝对路径"></a>1、路径表达式语法、相对/绝对路径</h4><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">路径表达式及描述</th></tr></thead><tbody><tr><td style="text-align:center">节点名称</td><td style="text-align:center">bookstore，选取 bookstore 下的所有子节点(标签)</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">/bookstore，从根节点下选取所有 bookstore 节点(子元素)</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">//bookstore，从全局节点中选择 bookstore 节点</td></tr><tr><td style="text-align:center">@</td><td style="text-align:center">//div[@price=‘a’]，选择所有 price 属性为 a 的 div</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">./input，选择当前节点下的 input</td></tr></tbody></table><h4 id="2、谓语"><a href="#2、谓语" class="headerlink" title="2、谓语"></a>2、谓语</h4><p>html 节点中第一个节点为 1，第二个为 2（需要区分）</p><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">//ul/li[1]</td><td style="text-align:center">选择 ul 下的第一个 li</td></tr><tr><td style="text-align:center">//ul/li[last()-0]</td><td style="text-align:center">选择 ul 下的最后一个 li</td></tr><tr><td style="text-align:center">//ul/li[last()-1]</td><td style="text-align:center">选择 ul 下的倒数第二个 li</td></tr><tr><td style="text-align:center">//ul/li[position()&lt;4]</td><td style="text-align:center">选择 ul 下前面的 3 个子元素</td></tr><tr><td style="text-align:center">//ul/li[position()&gt;1]</td><td style="text-align:center">选择第二个到最后的所有子元素</td></tr><tr><td style="text-align:center">//li[position()&gt;1] [position()&lt;11]</td><td style="text-align:center">在(2,+∞)中选择前十个</td></tr><tr><td style="text-align:center">text()</td><td style="text-align:center">获取函数文本</td></tr><tr><td style="text-align:center">@class</td><td style="text-align:center">获取标签的class</td></tr></tbody></table><img src="/blog/2019/03/13/xpath/2.png"><h4 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h4><table><thead><tr><th style="text-align:center">*</th><th style="text-align:center">/bookstore/*，通配符，匹配 bookstore 下的所有子元素</th></tr></thead><tbody><tr><td style="text-align:center">@*</td><td style="text-align:center">//div[@*]，选择所有带有属性的 div</td></tr></tbody></table><h4 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h4><ul><li>“|“    —&gt;    //title | //ul[@class=‘item_con_list’]，选择 title 和对应的 ul</li></ul><h3 id="四、使用lxml-amp-xpath解析html"><a href="#四、使用lxml-amp-xpath解析html" class="headerlink" title="四、使用lxml&amp;xpath解析html"></a>四、使用lxml&amp;xpath解析html</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">html1 = etree.parse(index.html)<span class="comment"># 可以通过读取html文件的方式</span></span><br><span class="line">html2 = etree.HTML(text)<span class="comment"># 也可以将字符串解析为HTML文档</span></span><br><span class="line"></span><br><span class="line">result1 = etree.tostring(html2)<span class="comment"># 将字符串序列化成HTML文档,会自动补全</span></span><br><span class="line"></span><br><span class="line">result2 = html2.xpath(<span class="string">'表达式'</span>)  <span class="comment"># 使用xpath语法</span></span><br></pre></td></tr></table></figure><h3 id="五、Example"><a href="#五、Example" class="headerlink" title="五、Example"></a>五、Example</h3><ul><li>以<a href="https://hr.tencent.com/position.php?lid=2218&amp;start=0#a" target="_blank" rel="noopener">腾讯招聘网</a>为例</li><li>我们要获取到职位名称、职位类别、人数、地点和发布时间等内容</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span>table<span class="regexp">//</span>tr</span><br></pre></td></tr></table></figure><p>选择到了13个子元素，分别是表头，翻页和底部其他招聘</p><img src="/blog/2019/03/13/xpath/3.png"><img src="/blog/2019/03/13/xpath/4.png">    <ul><li>那么可以往上找父元素，扩大范围</li></ul><img src="/blog/2019/03/13/xpath/5.png">    <figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//table[@class=<span class="string">'tablelist'</span>]//<span class="keyword">tr</span>[@class=<span class="string">'even'</span>] | <span class="regexp">//table</span>[@class=<span class="string">'tablelist'</span>]//<span class="keyword">tr</span>[@class=<span class="string">'odd'</span>]</span><br></pre></td></tr></table></figure><ul><li>使用以上表达式，避开表头和翻页</li><li>上述表达式虽然精确但是有点冗长，鉴于网站规律性，可以采用以下表达式</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//table[<span class="string">@class='tablelist'</span>]//tr[<span class="string">position()&gt;1</span>][<span class="symbol">position()&lt;11</span>]</span><br></pre></td></tr></table></figure><ul><li>上面的表达式写在程序里不加text()来取的话，会返回类似 <element tr="" at="" 0x1df36b5bc08=""> 的结果。</element></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'</span></span><br><span class="line">html = requests.get(url=url,headers=headers)<span class="comment"># html.text未经过编码的字符串，unicode字符串</span></span><br><span class="line">etree_obj = etree.HTML(html.text)<span class="comment"># HTML解析的是字符串，所以html.text</span></span><br><span class="line">result1 = etree_obj.xpath(<span class="string">"//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()"</span>)</span><br><span class="line">result2 = etree_obj.xpath(<span class="string">"//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]"</span>)</span><br><span class="line">print(result1)</span><br><span class="line">print(result2)</span><br></pre></td></tr></table></figure><img src="/blog/2019/03/13/xpath/6.png"><ul><li><p>结果可以看到有许多的转义字符，比如：\r、\t</p></li><li><p>最后将代码处理一下，照此方法可以获取到腾讯招聘的所有职位信息(后面有个坑，比如职位类别是空数据的话，XPath匹配到会自动放弃)</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'</span></span><br><span class="line">html = requests.get(url=url,headers=headers)<span class="comment"># html.text未经过编码的字符串，unicode字符串</span></span><br><span class="line">etree_obj = etree.HTML(html.text)<span class="comment"># HTML解析的是字符串，所以html.text</span></span><br><span class="line">result = etree_obj.xpath(<span class="string">"//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()"</span>)</span><br><span class="line">print(result)</span><br><span class="line">result2 = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> result <span class="keyword">if</span> x.strip() != <span class="string">''</span>]</span><br><span class="line">print(result2)</span><br><span class="line">result3 = [print(result2[x],result2[x+<span class="number">1</span>],result2[x+<span class="number">2</span>],result2[x+<span class="number">3</span>],result2[x+<span class="number">4</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,len(result2),<span class="number">5</span>)]</span><br><span class="line">print(result3)</span><br></pre></td></tr></table></figure><ul><li>结果图</li></ul><img src="/blog/2019/03/13/xpath/7.png"><ul><li><h4 id="本篇代码Github地址：https-github-com-Coder-Sakura-exp-tree-master-xpath"><a href="#本篇代码Github地址：https-github-com-Coder-Sakura-exp-tree-master-xpath" class="headerlink" title="本篇代码Github地址：https://github.com/Coder-Sakura/exp/tree/master/xpath"></a>本篇代码Github地址：<a href="https://github.com/Coder-Sakura/exp/tree/master/xpath" target="_blank" rel="noopener">https://github.com/Coder-Sakura/exp/tree/master/xpath</a></h4></li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> Xpath </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于git的操作的一些记录</title>
      <link href="/blog/2019/03/03/git-note/"/>
      <url>/blog/2019/03/03/git-note/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/03/03/git-note/HeadPicture.png"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：主要记录git的命令和一些git的知识</p></blockquote><a id="more"></a><h3 id="一、github介绍"><a href="#一、github介绍" class="headerlink" title="一、github介绍"></a>一、github介绍</h3><p><a href="https://github.com/" target="_blank" rel="noopener">github</a>(<del>基佬站</del>)是一个开源代码托管平台(其中当然也有私有项目)，也作为一个版本控制系统，让你对代码的版本控制更加简单，不用去担心代码写错了怎么办？有没有备份？专心自己的项目就好。</p><p>本文没有关于桌面版的git安装、环境变量配置的教程(安装配置的话百度有很多教程)</p><img src="/blog/2019/03/03/git-note/1.jpg"><h3 id="二、github功能"><a href="#二、github功能" class="headerlink" title="二、github功能"></a>二、github功能</h3><ol><li>可以在上面找到许多开源项目、脚本甚至可以在上面找到一些课程</li><li>托管项目。只要连上互联网就可以同步到自己的项目代码或多人跟进项目</li><li>利用github和一些开源的博客系统可以搭建个人博客(本博客是hexo+github搭建的)</li></ol><h3 id="三、git命令"><a href="#三、git命令" class="headerlink" title="三、git命令"></a>三、git命令</h3><ol><li><font color="#FF3030">设置用户名和邮箱</font>，不设置会报“please tell me who you are.”，–global参数表示全局</li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"Your Name"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.email <span class="string">"email@example.com"</span></span><br><span class="line">git<span class="built_in"> config </span>--list # 检查设置</span><br></pre></td></tr></table></figure><ol start="2"><li>初始化本地文件夹为git仓库（会生成.git隐藏文件，主要是用于版本控制）</li></ol><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git init</span></span><br></pre></td></tr></table></figure><ol start="3"><li>本地版本管理</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">git add ./[name]<span class="comment"># 跟踪文件进入暂存区，.表示当前目录所有文件，也可以指定文件</span></span><br><span class="line">git status<span class="comment"># 命令用于显示工作目录和暂存区的状态</span></span><br><span class="line">git <span class="keyword">commit</span> -m <span class="string">'提交说明'</span> <span class="comment"># 将暂存区里的改动给提交到本地的版本库</span></span><br><span class="line">git <span class="keyword">log</span> <span class="comment">--pretty=oneline# 查看最近到最远的提交日志，oneline表示每条输出一行</span></span><br><span class="line"><span class="comment">#  $ git log --pretty=oneline</span></span><br><span class="line"><span class="comment">#  f3e98b7f4495c78bf98f2661fad2ae745cd60b63 (HEAD -&gt; master, origin/master) proxy</span></span><br><span class="line"><span class="comment">#  f3e9....这串就是这次提交的版本号</span></span><br><span class="line">git <span class="keyword">reset</span> <span class="comment">--hard [版本号] # 回退/前进到某个指定版本，版本号可以在git log中找到</span></span><br><span class="line"><span class="comment">#  也有快捷的回退命令</span></span><br><span class="line">git <span class="keyword">reset</span> <span class="comment">--hard HEAD^# 回退到上个版本</span></span><br><span class="line">git <span class="keyword">reset</span> <span class="comment">--hard HEAD^^# 回退到上上个版本</span></span><br><span class="line">git <span class="keyword">reset</span> <span class="comment">--hard HEAD~100# 回退到上100个版本</span></span><br></pre></td></tr></table></figure><ol start="3"><li>将本地文件提交到github</li></ol><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git add ./(name)</span><br><span class="line">git commit -m <span class="string">'message'</span></span><br><span class="line">git remote add origin [github仓库地址]</span><br><span class="line"><span class="meta"># 如果出现错误：fatal: remote origin already exists</span></span><br><span class="line"><span class="meta"># 执行 git remote rm origin#删除分支</span></span><br><span class="line"><span class="meta"># 再执行 git remote add origin [github仓库地址]#再添加</span></span><br><span class="line">git push origin master# 推送到github仓库</span><br><span class="line"><span class="meta"># 如果出现failed to push som refs to…….</span></span><br><span class="line"><span class="meta"># 需要将github仓库的文件同步下来先</span></span><br><span class="line"><span class="meta"># git pull origin master# pull拉文件下来</span></span><br><span class="line"><span class="meta"># 再执行 git push origin master# push推文件上去</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python爬虫[代理]</title>
      <link href="/blog/2019/02/28/proxy/"/>
      <url>/blog/2019/02/28/proxy/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/02/28/proxy/HeadPicture.png"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：代理原理作用，requests设置代理方法以及爬取免费代理的脚本实例</p></blockquote><a id="more"></a><h3 id="一、代理原理"><a href="#一、代理原理" class="headerlink" title="一、代理原理"></a>一、代理原理</h3><img src="/blog/2019/02/28/proxy/proxy_1.png"><p>根据自己理解解读：</p><ol><li>客户端设置了代理信息后，客户端向对应的代理站点发出请求（向xxx网站发起请求）</li><li>代理站点收到请求之后，就会执行对应的响应动作（执行动作）</li><li>代理站点获得xxx网站的响应（得到站点响应）</li><li>代理站点根据客户端要求返回对应信息（客户端要求返回Source code，则返回Source code）</li></ol><h3 id="二、代理作用"><a href="#二、代理作用" class="headerlink" title="二、代理作用"></a>二、代理作用</h3><ol><li>突破自身ip访问限制，比如访问国外站点</li><li>爬取对ip访问频率有一定限制的站点</li><li>提高访问速度</li><li>隐藏真实ip</li></ol><h3 id="三、代理网站"><a href="#三、代理网站" class="headerlink" title="三、代理网站"></a>三、代理网站</h3><p><strong>免费代理ip列表</strong>：</p><table><thead><tr><th>含国外ip</th><th><a href="https://ip.seofangfa.com/" target="_blank" rel="noopener">方法SEO顾问</a>，<a href="http://www.89ip.cn/" target="_blank" rel="noopener">89代理</a>，<a href="https://ip.ihuan.me/" target="_blank" rel="noopener">小幻http代理</a>，<a href="http://www.ip3366.net/" target="_blank" rel="noopener">云代理</a></th></tr></thead><tbody><tr><td>不含</td><td><a href="https://www.xicidaili.com/" target="_blank" rel="noopener">西刺</a>，<a href="https://www.kuaidaili.com/" target="_blank" rel="noopener">快代理</a></td></tr></tbody></table><p><strong>付费代理尚未了解，此处留空</strong></p><h3 id="四、requests设置代理方法"><a href="#四、requests设置代理方法" class="headerlink" title="四、requests设置代理方法"></a>四、requests设置代理方法</h3><p>requests中有预设好的参数接收代理信息 proxies，这个参数接收的是一个字典对象</p><p>因为不知道访问的网站使用的是http协议还是https协议，所以proxies最好2种都有设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>:<span class="number">92.255</span><span class="number">.255</span><span class="number">.78</span>:<span class="number">54628</span>,</span><br><span class="line">    <span class="string">'https'</span>:<span class="number">92.255</span><span class="number">.255</span><span class="number">.78</span>:<span class="number">54628</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url=url1,headers=headers,proxies=proxies)</span><br></pre></td></tr></table></figure><h3 id="五、脚本示例"><a href="#五、脚本示例" class="headerlink" title="五、脚本示例"></a>五、脚本示例</h3><p>github地址：<a href="https://github.com/Coder-Sakura/exp/tree/master/seo_ip" target="_blank" rel="noopener">https://github.com/Coder-Sakura/exp/tree/master/seo_ip</a></p><p>本来我是打算用89代理的api接口，但是测试之后发现可靠性有点低，并且外网ip比较少，所以转用<a href="https://ip.seofangfa.com/" target="_blank" rel="noopener">SEO</a></p><p>（本次抓取代理ip主要是用在我自己做 <a href="http://www.pixiv.net" target="_blank" rel="noopener">pixiv</a> 的小项目上，爬取关注画师的所有作品和自己的收藏作品，后续会整理出来，初学爬虫，有错还请指正）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> requests.packages.urllib3.exceptions <span class="keyword">import</span> InsecureRequestWarning     <span class="comment"># 用于强制取消警告</span></span><br><span class="line"><span class="keyword">from</span> requests.adapters <span class="keyword">import</span> HTTPAdapter                                   <span class="comment"># 用于强制取消警告</span></span><br><span class="line"></span><br><span class="line">requests.packages.urllib3.disable_warnings(InsecureRequestWarning)      <span class="comment"># 强制取消警告</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">seo_ip</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) '</span></span><br><span class="line">                          <span class="string">'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>&#125;</span><br><span class="line">        self.agent_ip_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Agent</span><span class="params">(self,ip_agent_url)</span>:</span></span><br><span class="line">        html = requests.get(url=ip_agent_url,headers=self.headers,verify=<span class="keyword">False</span>,timeout=<span class="number">5</span>)</span><br><span class="line">        html_soup = BeautifulSoup(html.text, <span class="string">'lxml'</span>)</span><br><span class="line">        <span class="comment"># 去除第一个和前25个，26-50为国外ip</span></span><br><span class="line">        ip_list = html_soup.find(<span class="string">'tbody'</span>).find_all(<span class="string">'tr'</span>)[<span class="number">26</span>:]    </span><br><span class="line">        items = []</span><br><span class="line">        print(<span class="string">'搜索完成,代理信息如下:'</span>) </span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> ip_list:        </span><br><span class="line">            ip_port = list(item)[<span class="number">0</span>].get_text() + <span class="string">':'</span> +list(item)[<span class="number">1</span>].get_text()</span><br><span class="line">            <span class="comment"># list(ip_port)[0]为ip,[1]为端口,[2]响应时间,[3]位置,[4]最后验证时间</span></span><br><span class="line">            print(<span class="string">'ip: %s ,响应时间: %ss ,ip位置: %s'</span> % (ip_port,list(item)[<span class="number">2</span>].get_text(),list(item)[<span class="number">3</span>].get_text()))</span><br><span class="line">            items.append(ip_port)        <span class="comment">#存储爬取到的ip(需要添加)</span></span><br><span class="line">        <span class="keyword">return</span> items</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge</span><span class="params">(self,items)</span>:</span>       <span class="comment"># 检验ip活性     # https://ip.seofangfa.com/</span></span><br><span class="line">        print(<span class="string">'正在进行代理池ip活性检测......'</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                proxy = &#123;</span><br><span class="line">                    <span class="string">'http'</span>:item,</span><br><span class="line">                    <span class="string">'https'</span>:item</span><br><span class="line">                    &#125;</span><br><span class="line">                <span class="comment"># 遍历时，利用百度，设定timeout，未响应则断开连接</span></span><br><span class="line">                judge_url = <span class="string">'https://www.baidu.com/'</span>     </span><br><span class="line">                response = requests.get(url=judge_url,headers=self.headers,proxies=proxy,verify=<span class="keyword">False</span>,timeout=<span class="number">5</span>)</span><br><span class="line">                self.agent_ip_list.append(item)</span><br><span class="line">                print(item,<span class="string">'可用...'</span>)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(item,<span class="string">'不可用...'</span>)</span><br><span class="line">        print(<span class="string">'代理池ip活性检测完毕...\n代理池总量:'</span>,len(self.agent_ip_list),<span class="string">'\n代理池:'</span>,self.agent_ip_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></span><br><span class="line">        ip_agent_url = <span class="string">'https://ip.seofangfa.com/'</span></span><br><span class="line">        items = self.Agent(ip_agent_url)</span><br><span class="line">        self.judge(items)</span><br><span class="line"></span><br><span class="line">seo_ip = seo_ip()</span><br><span class="line">seo_ip.work()</span><br></pre></td></tr></table></figure><p>没有导入这2个库的话，会因为ssl证书而出现警告，如图：</p><ol><li>from requests.packages.urllib3.exceptions import InsecureRequestWarning</li><li>from requests.adapters import HTTPAdapter</li></ol><img src="/blog/2019/02/28/proxy/proxy_2.png"><h3 id="六、附图"><a href="#六、附图" class="headerlink" title="六、附图"></a>六、附图</h3><blockquote><p> 最后附上运行图</p></blockquote><img src="/blog/2019/02/28/proxy/proxy_3.png">]]></content>
      
      
      <categories>
          
          <category> proxy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python第三方库安装[pip、whl]</title>
      <link href="/blog/2019/02/12/212/"/>
      <url>/blog/2019/02/12/212/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/02/12/212/HeadPicture.jpg"><hr><blockquote><h2 id="说在前面的话"><a href="#说在前面的话" class="headerlink" title="说在前面的话"></a>说在前面的话</h2><p>本篇概述：python第三方库安装</p></blockquote><a id="more"></a><h3 id="一、pip安装"><a href="#一、pip安装" class="headerlink" title="一、pip安装"></a>一、pip安装</h3><ul><li>pip3 install [库名] 或 pip install [库名]</li></ul><hr><blockquote><h4 id="2019-02-28更新"><a href="#2019-02-28更新" class="headerlink" title="[2019.02.28更新]"></a>[2019.02.28更新]</h4></blockquote><p>针对 ‘pip’ 不是内部命令，也不是可运行的程序的情况：</p><p>原因：环境变量 Path 未配置完成</p><ol><li>找到 python 的安装目录，将 <font color="#FF3030">python 的安装目录</font> 和 <font color="#FF3030">Python安装目录\Scripts</font> 添加到 环境变量 Path 中即可；</li><li>环境变量Path：计算机属性 -&gt; 高级系统设置 -&gt; 高级 -&gt; 环境变量</li></ol><img src="/blog/2019/02/12/212/3.png"><h3 id="二、whl安装"><a href="#二、whl安装" class="headerlink" title="二、whl安装"></a>二、whl安装</h3><div><div class="fold_hider"><div class="close hider_title"><u>点我可以将内容伸缩哦~</u></div></div><div class="fold"><p>٩(๑&gt;◡&lt;๑)۶</p><blockquote><ul><li>如果pip安装不行，可以考虑whl安装(轮子大法好！) </li><li><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">Python常用库whl文件下载</a></li><li>如何知道本机安装的python支持哪个版本的轮子?</li></ul></blockquote><h5 id="首先要知道系统是多少位的？-在cmd中输入"><a href="#首先要知道系统是多少位的？-在cmd中输入" class="headerlink" title="首先要知道系统是多少位的？(在cmd中输入)"></a>首先要知道系统是多少位的？(在cmd中输入)</h5><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systeminfo <span class="string">| findstr "</span>系统类型<span class="string">"</span></span><br></pre></td></tr></table></figure><p>结果：x64-based PC  = (64位 AMD64)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pip._internal</span><br><span class="line">print(pip._internal.pep425tags.get_supported())</span><br></pre></td></tr></table></figure><p>X86-based PC  = (32位 WIN32)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pip</span><br><span class="line">print(pip.pep425tags.get_supported())</span><br></pre></td></tr></table></figure><h5 id="选择对应的轮子"><a href="#选择对应的轮子" class="headerlink" title="选择对应的轮子"></a>选择对应的轮子</h5><ol><li><p>输入上面的代码后，会返回一个list，list里面就是当前系统支持的whl版本</p></li><li><p>比如 ‘cp37’, ‘cp37m’, ‘win_amd64’，cp37对应的是python3.7版本; cp37m 对应的是依赖于python3.7应用程序二进制接口; win_amd64对应的是64位系统编译的。</p></li></ol><p><img src="/blog/2019/02/12/212/1.png"></p><ol start="3"><li>打开上面的网址，ctrl + F ，这里使用 mysqlclient 作为示范</li></ol><p><img src="/blog/2019/02/12/212/2.png"></p><ol start="4"><li><p>根据刚刚的结果，下载以下版本的轮子即可。</p><p><strong>mysqlclient‑1.4.2‑cp37‑cp37m‑win_amd64.whl</strong></p></li></ol><p>​    库名 - 版本号 - 对应python版本 - 依赖 - 系统位数</p><h5 id="安装轮子"><a href="#安装轮子" class="headerlink" title="安装轮子"></a>安装轮子</h5><ol start="5"><li>进入到轮子目录，cmd打开，pip install [名字].whl 即可</li></ol><hr><blockquote><h4 id="2019-02-28更新"><a href="#2019-02-28更新" class="headerlink" title="[2019.02.28更新]"></a>[2019.02.28更新]</h4></blockquote><p>针对 pip install [名字].whl 安装不成功的情况</p><ol><li>可以将whl文件的后缀名.whl更改为.zip，然后解压</li><li>在解压目录下进行 python setup.py install 运行安装[通常whl文件解压后都有setup.py]</li><li>对于没有 setup.py 的，直接将解压目录放入libs文件夹中</li><li>whl文件是已经编译好的文件，作用主要是为了方便我们进行 python 的第三方库安装和使用</li></ol></div></div><h3 id="三、Anaconda"><a href="#三、Anaconda" class="headerlink" title="三、Anaconda"></a>三、Anaconda</h3><p>​    Anaconda包括Conda、Python以及一大堆安装好的科学包和依赖项。(Conda是一个开源的包和环境的管理器)</p><p>​    从 <a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">Anaconda官网</a> 下载，图形化安装，十分简单，而且网上的教程也多。</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 第三方库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/blog/2019/02/09/hello-world/"/>
      <url>/blog/2019/02/09/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.    </p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
